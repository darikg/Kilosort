{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc55352-4b1d-4d0f-a325-83bf481234eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a85ac5-3415-4c00-a3a7-27d062198e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(r\"D:\\recording\\2024-05-02-13-37-29\")\n",
    "results_dir = root / 'kilosort4'\n",
    "\n",
    "# One value per cluster\n",
    "camps = pd.read_csv(results_dir / 'cluster_Amplitude.tsv', sep='\\t')['Amplitude'].values\n",
    "clu_labels = pd.read_csv(results_dir / 'cluster_KSLabel.tsv', sep='\\t')['KSLabel'].values\n",
    "contam_pct = pd.read_csv(results_dir / 'cluster_ContamPct.tsv', sep='\\t')['ContamPct'].values\n",
    "chan_map =  np.load(results_dir / 'channel_map.npy')\n",
    "templates =  np.load(results_dir / 'templates.npy')\n",
    "chan_best = (templates**2).sum(axis=1).argmax(axis=-1)\n",
    "chan_best = chan_map[chan_best]\n",
    "\n",
    "# One value per spike\n",
    "amplitudes = np.load(results_dir / 'amplitudes.npy')\n",
    "st = np.load(results_dir / 'spike_times.npy')\n",
    "clu = np.load(results_dir / 'spike_clusters.npy')\n",
    "\n",
    "sr = 20_000\n",
    "\n",
    "subfolders, n_samples = [], []\n",
    "with open(root / 'concatenated.csv', newline='') as f:\n",
    "    csvfile = csv.reader(f)\n",
    "    _ = next(csvfile)  # skip header\n",
    "    for (subfolder, n) in csvfile:\n",
    "        subfolders.append(subfolder)\n",
    "        n_samples.append(int(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57474227-2e87-444c-b8a3-dc6aa9ebdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = np.array(n_samples)\n",
    "cum_samples = np.cumsum(n_samples)\n",
    "cum_samples0 = np.append(0, cum_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21dcb901-4034-466e-a989-0875c9408047",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(\n",
    "    user='xper_rw',\n",
    "    password='up2nite',\n",
    "    host='172.30.6.54',\n",
    "    database='SpikeData',\n",
    "    client_flags=[mysql.connector.constants.ClientFlag.LOCAL_FILES],\n",
    "    allow_local_infile=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ab08d0-0093-4425-af3f-087d101da7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notefiles = [f.replace('experiment', 'notes') + '.txt' for f in subfolders]\n",
    "nf_str = ', '.join(f'\"{f}\"' for f in notefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0945bec2-0d69-445d-821c-d320d9db7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = db.cursor()\n",
    "curs.execute('INSERT INTO ClusteredSessions () VALUES ()')\n",
    "curs.execute('SELECT MAX(session_id) FROM ClusteredSessions')\n",
    "session_id, = curs.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fd4a4b-41d1-4a2f-99da-869fa49444e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curs.execute(f\"SELECT recording_id FROM Recordings WHERE notefile IN ({nf_str})\")\n",
    "recording_ids = [i for i, in curs.fetchall()]\n",
    "rstr = ', '.join(str(i) for i in recording_ids)\n",
    "curs.execute(f\"DELETE FROM ClusteredRuns WHERE recording_id IN ({rstr})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2d7580-691c-4096-b3d0-5f6becec0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"INSERT INTO ClusteredRuns (session_id, recording_id) VALUES (%s, %s)\"\n",
    "curs.executemany(q, [(session_id, rid) for rid in recording_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2d7a53-b320-48da-808a-ef676797405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"INSERT INTO Clusters (session_id, cluster, amplitude, contam_pct, best_channel, label) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "data = [(session_id, i, amp, cptc, int(best_ch), lbl) for i, (amp, cptc, best_ch, lbl) in enumerate(zip(camps, contam_pct, chan_best, clu_labels))]\n",
    "curs.executemany(q, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f365a23a-e0c4-4dd1-bd58-e26543bf7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curs.execute(f\"\"\"\n",
    "    SELECT    notefile, trial_id, start_sample, stop_sample \n",
    "    FROM      Trials INNER JOIN Recordings USING (recording_id)\n",
    "    WHERE     notefile IN ({nf_str})\n",
    "    ORDER BY  recording_id, start_sample\n",
    "\"\"\")\n",
    "trials_df = pd.DataFrame.from_records(curs.fetchall(), columns=['notefile', 'trial_id', 'start_sample', 'stop_sample'])\n",
    "trials_df['folder_idx'] = trials_df.notefile.apply(lambda f: notefiles.index(f))\n",
    "trials_df.drop('notefile', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9585b5-287c-4e9c-8867-cecd295e5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_idx = np.searchsorted(cum_samples, st)\n",
    "spikes_df = pd.DataFrame(dict(\n",
    "    spike_time=st - cum_samples0[folder_idx], \n",
    "    folder_idx=folder_idx,\n",
    "    cluster_idx=clu,\n",
    "    amplitude=amplitudes,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9428c0f9-b241-41c0-8a38-bee78f91be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_recording(spks, trials, temp_csv_file = r\"data.csv\"):\n",
    "    trials['trial_idx'] = np.arange(len(trials))\n",
    "    spks['trial_idx'] = np.searchsorted(trials.start_sample, spks.spike_time) - 1\n",
    "    trial_spks = spks.join(trials, how='inner', on='trial_idx', lsuffix='l_', rsuffix='r_')\n",
    "    trial_spks.query('start_sample < spike_time <= stop_sample')\n",
    "    trial_spks['spike_time'] = (trial_spks.spike_time - trial_spks.start_sample) / sr\n",
    "    trial_spks = trial_spks.loc[:, ['trial_id', 'cluster_idx', 'spike_time', 'amplitude']]\n",
    "    trial_spks.rename(columns=dict(cluster_idx='channel', spike_time='time'), inplace=True)\n",
    "    trial_spks.to_csv(temp_csv_file, index=False)\n",
    "    \n",
    "    trial_id_str = ','.join(str(i) for i in trials.trial_id.unique())\n",
    "    q = f'DELETE FROM SortedSpikeTimes WHERE trial_id IN ({trial_id_str})'\n",
    "    curs.execute(q)\n",
    "    \n",
    "    q = rf\"\"\"\n",
    "        LOAD DATA LOCAL INFILE '{temp_csv_file}' \n",
    "        INTO TABLE SortedSpikeTimes\n",
    "        FIELDS TERMINATED BY ','\n",
    "        LINES TERMINATED by '\\r\\n'\n",
    "        IGNORE 1 LINES\n",
    "    \"\"\"\n",
    "    curs.execute(q)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fcde74-683f-4068-9011-cca6afc91f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(subfolders)):\n",
    "    write_recording(\n",
    "        spks=spikes_df[spikes_df.folder_idx == i].drop('folder_idx', axis=1),\n",
    "        trials=trials_df[trials_df.folder_idx == i].drop('folder_idx', axis=1),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
